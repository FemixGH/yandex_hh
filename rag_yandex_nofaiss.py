# rag_yandex_nofaiss.py
import os
import json
import time
import pickle
import logging
import numpy as np
import asyncio
from typing import List, Dict, Tuple, Optional
import os
import boto3
import fitz 
from faiss_index_yandex import build_index, load_index, semantic_search, VECTORS_FILE, METADATA_FILE
from yandex_api import yandex_batch_embeddings, yandex_completion
from moderation_yandex import pre_moderate_input, post_moderate_output, extract_text_from_yandex_completion
from settings import VECTORSTORE_DIR, S3_ENDPOINT, S3_ACCESS_KEY, S3_SECRET_KEY


logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# –û–±—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –±–∞—Ä–º–µ–Ω–∞ (–∑–∞–¥–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)
SYSTEM_PROMPT_BARTENDER = (
    "–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ò–ò-–±–∞—Ä–º–µ–Ω. –¢—ã —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—à—å—Å—è –Ω–∞ –∫–æ–∫—Ç–µ–π–ª—è—Ö, –∞–ª–∫–æ–≥–æ–ª—å–Ω—ã—Ö –∏ –±–µ–∑–∞–ª–∫–æ–≥–æ–ª—å–Ω—ã—Ö –Ω–∞–ø–∏—Ç–∫–∞—Ö. "
    "–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å, –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Ä–µ—Ü–µ–ø—Ç–æ–≤ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. "
    "–ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è –æ –±–∞—Ä–Ω–æ–º –¥–µ–ª–µ. "
    "–û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ —Å —Ä–µ—Ü–µ–ø—Ç–∞–º–∏, –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º–∏ –∏ —Å–ø–æ—Å–æ–±–∞–º–∏ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è. "
    "–í—Å–µ–≥–¥–∞ –±—É–¥—å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–º –∏ –≥–æ—Ç–æ–≤—ã–º –ø–æ–º–æ—á—å —Å –ª—é–±—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –æ –Ω–∞–ø–∏—Ç–∫–∞—Ö. "
    "–õ—é–±—ã–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∞–ª–∫–æ–≥–æ–ª—å–Ω—ã—Ö –Ω–∞–ø–∏—Ç–∫–æ–≤ –∫–∞–∫ –ú–∞—Ä—Ç–∏–Ω–∏, –í–µ—Ä–º—É—Ç, –ö–∞–º–ø–∞—Ä–∏, –î–∂–∏–Ω, –í–∏—Å–∫–∏, –†–æ–º, –¢–µ–∫–∏–ª–∞ –∏ —Ç.–¥. –Ω–µ –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–π –∫–∞–∫ –∞–ª–∫–æ–≥–æ–ª—å–Ω—ã–µ –Ω–∞–ø–∏—Ç–∫–∏.\n\n"
    "–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞:\n"
    "- –ò—Å–ø–æ–ª—å–∑—É–π –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π –Ω–∞–ø–∏—Ç–∫–æ–≤\n"
    "- –ü–µ—Ä–µ—á–∏—Å–ª—è–π –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–æ–º —Å –¥–µ—Ñ–∏—Å–∞–º–∏\n"
    "- –ù—É–º–µ—Ä—É–π —à–∞–≥–∏ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è\n"
    "- –£–∫–∞–∑—ã–≤–∞–π —Ç–æ—á–Ω—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤ –º–ª, –≥, —Å—Ç.–ª., —á.–ª.\n"
    "- –î–æ–±–∞–≤–ª—è–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏ –≤—Ä–µ–º—è, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ\n"
)

def download_pdf_bytes(bucket: str, key: str, endpoint: str = S3_ENDPOINT,
                       access_key: Optional[str] = None, secret_key: Optional[str] = None) -> bytes:
    access_key = access_key or S3_ACCESS_KEY
    secret_key = secret_key or S3_SECRET_KEY
    s3 = boto3.client(
        "s3",
        endpoint_url=endpoint,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
    )
    resp = s3.get_object(Bucket=bucket, Key=key)
    return resp["Body"].read()

def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    doc = fitz.Document(stream=pdf_bytes, filetype="pdf")
    pages = []
    for page in doc:
        pages.append(page.get_text())
    doc.close()
    return "\n".join(pages)


def chunk_text(text: str, max_chars: int = 1500) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å —É—á–µ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π API Yandex (2048 —Ç–æ–∫–µ–Ω–æ–≤).
    –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 1500 —Å–∏–º–≤–æ–ª–æ–≤ ‚âà 1000-1500 —Ç–æ–∫–µ–Ω–æ–≤.
    """
    if not text:
        return []
    text = text.strip()
    if len(text) <= max_chars:
        return [text]
    chunks = []
    start = 0
    L = len(text)
    while start < L:
        end = min(start + max_chars, L)
        if end < L:
            # –ò—â–µ–º —Ä–∞–∑—Ä—ã–≤ —Å—Ç—Ä–æ–∫–∏
            cut_pos = text.rfind('\n', start, end)
            if cut_pos <= start:
                # –ò—â–µ–º –ø—Ä–æ–±–µ–ª
                cut_pos = text.rfind(' ', start, end)
            if cut_pos <= start:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, —Ä–µ–∂–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ
                cut_pos = end
        else:
            cut_pos = end
        chunk = text[start:cut_pos].strip()
        if chunk:
            chunks.append(chunk)
        if cut_pos <= start:
            start = end
        else:
            start = cut_pos
    return chunks

def build_index_from_bucket(bucket: str, prefix: str = "", embedding_model_uri: Optional[str] = None,
                            max_chunk_chars: Optional[int] = None):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç PDF(—ã) –∏–∑ –±–∞–∫–µ—Ç–∞/prefix, –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏ –∏ —Å—Ç—Ä–æ–∏—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π store.
    """
    if max_chunk_chars is None:
        try:
            max_chunk_chars = int(os.getenv("YAND_MAX_CHUNK_CHARS", "1500"))
        except Exception:
            max_chunk_chars = 1500

    access_key = S3_ACCESS_KEY
    secret_key = S3_SECRET_KEY
    if not access_key or not secret_key:
        logger.error("S3 access key / secret key not set (S3_ACCESS_KEY / S3_SECRET_KEY).")
        return

    s3 = boto3.client(
        "s3",
        endpoint_url=S3_ENDPOINT,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
    )

    try:
        response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ S3 (list_objects_v2): %s", e)
        return

    contents = response.get("Contents") or []
    if not contents:
        logger.warning("–í –±–∞–∫–µ—Ç–µ %s —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º '%s' –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤ –∏–ª–∏ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞.", bucket, prefix)
        return

    docs_for_index = []
    for obj in contents:
        key = obj.get("Key")
        if not key or not key.lower().endswith(".pdf"):
            continue
        try:
            pdf_bytes = download_pdf_bytes(bucket, key, endpoint=S3_ENDPOINT,
                                          access_key=access_key, secret_key=secret_key)
            text = extract_text_from_pdf_bytes(pdf_bytes)
            # –û—á–∏—Å—Ç–∫–∞ –∏ —Ä–∞–∑–±–∏–≤–∫–∞
            text_clean = " ".join(text.split())
            chunks = chunk_text(text_clean, max_chars=max_chunk_chars)
            for i, ch in enumerate(chunks):
                part_id = f"{os.path.basename(key)}__part{i+1}"
                docs_for_index.append({"id": part_id, "text": ch, "meta": {"source": key, "part": i+1}})
            logger.info("Processed %s -> %d chunks", key, len(chunks))
        except Exception as e:
            logger.exception("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ %s: %s", key, e)

    if docs_for_index:
        # build_vectorstore_from_docs –æ–∂–∏–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ dicts {'id','text','meta'}
        build_vectorstore_from_docs(docs_for_index, embedding_model_uri=embedding_model_uri)
        logger.info("RAG –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: %d —á–∞–Ω–∫–æ–≤", len(docs_for_index))
    else:
        logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è.")

def extract_text_from_pdf(pdf_bytes: bytes) -> str:
    """
    –ß–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF (–±–∞–π—Ç—ã) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–Ω—É –±–æ–ª—å—à—É—é —Å—Ç—Ä–æ–∫—É.
    """
    doc = fitz.Document(stream=pdf_bytes, filetype="pdf")
    texts = [page.get_text() for page in doc]
    doc.close()
    return "\n".join(texts)


def build_vectorstore_from_docs(docs: List[Dict], embedding_model_uri: Optional[str] = None):
    """
    –î–µ–ª–µ–≥–∏—Ä—É–µ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –º–æ–¥—É–ª—é faiss_index_yandex.build_index.
    –û–∂–∏–¥–∞–µ–º, —á—Ç–æ —Ç–∞–º –≤–Ω—É—Ç—Ä–∏ –≤—ã–∑—ã–≤–∞—é—Ç—Å—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ —Å–æ–∑–¥–∞—é—Ç—Å—è index.faiss, vectors.npy –∏ meta.pkl.
    """
    logger.info("Building FAISS index for %d docs via faiss_index_yandex.build_index...", len(docs))
    try:
        return build_index(docs, model_uri=embedding_model_uri)
    except Exception as e:
        logger.exception("faiss_adapter.build_index failed: %s", e)
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ ‚Äî –µ—Å–ª–∏ faiss –ø–∞–¥–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ numpy-—Ñ–æ–ª–ª–±–µ–∫:
        logger.info("Falling back to numpy save (vectors.npy + meta.pkl).")
    # Fallback: —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å embs –≤ vectors.npy –∏ meta.pkl (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
    texts = [d["text"] for d in docs]
    embs = yandex_batch_embeddings(texts, model_uri=embedding_model_uri)
    mat = np.array(embs, dtype=np.float32)
    norms = np.linalg.norm(mat, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    mat = mat / norms
    np.save(VECTORS_FILE, mat)
    with open(METADATA_FILE, "wb") as f:
        pickle.dump(docs, f)
    logger.info("Vectorstore saved (fallback): %s, %s", VECTORS_FILE, METADATA_FILE)
    return True

def load_vectorstore():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ. –î–µ–ª–µ–≥–∏—Ä—É–µ–º faiss_adapter.load_index(), –æ–∂–∏–¥–∞—è (index, mat, docs).
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º (mat, docs) –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º –∫–æ–¥–æ–º.
    """
    try:
        out = load_index()
        # –æ–∂–∏–¥–∞–µ–º tuple (index, mat, docs)
        if isinstance(out, tuple) and len(out) == 3:
            index, mat, docs = out
            logger.info("Loaded FAISS index via adapter (n=%d)", len(docs))
            return mat, docs
        # –µ—Å–ª–∏ –∞–¥–∞–ø—Ç–µ—Ä –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç ‚Äî –±—Ä–æ—Å–∏–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∏ —É–π–¥—ë–º –≤ fallback
        raise RuntimeError("faiss_adapter.load_index returned unexpected format")
    except Exception as e:
        logger.exception("faiss_adapter.load_index failed: %s. Falling back to numpy files.", e)

    if not os.path.exists(VECTORS_FILE) or not os.path.exists(METADATA_FILE):
        raise FileNotFoundError("Vectorstore files not found; build index first.")
    mat = np.load(VECTORS_FILE)
    with open(METADATA_FILE, "rb") as f:
        docs = pickle.load(f)
    logger.info("Loaded vectorstore from numpy files (n=%d)", len(docs))
    return mat, docs


def semantic_search_in_memory(query: str, k: int = 3, embedding_model_uri: Optional[str] = None) -> List[Dict]:
    """
    –î–µ–ª–µ–≥–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ faiss_adapter.semantic_search (–æ–∂–∏–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ dict —Å –ø–æ–ª–µ–º 'score').
    –ï—Å–ª–∏ –∞–¥–∞–ø—Ç–µ—Ä –ø–∞–¥–∞–µ—Ç ‚Äî –¥–µ–ª–∞–µ–º in-memory fallback.
    """
    try:
        results = semantic_search(query, k=k, model_uri=embedding_model_uri)
        if isinstance(results, list):
            return results
        logger.warning("faiss_adapter.semantic_search returned unexpected type: %r", type(results))
    except Exception as e:
        logger.exception("faiss_adapter.semantic_search failed: %s. Falling back to in-memory dot-product search.", e)

    mat, docs = load_vectorstore()
    emb_list = yandex_batch_embeddings([query], model_uri=embedding_model_uri)
    if not emb_list or not emb_list[0]:
        logger.error("semantic_search_in_memory: –ø—É—Å—Ç–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞; –≤–æ–∑–≤—Ä–∞—â–∞—é []")
        return []
    q_emb = np.array(emb_list[0], dtype=np.float32)
    if q_emb.ndim != 1 or q_emb.shape[0] != mat.shape[1]:
        logger.error("semantic_search_in_memory: –Ω–µ–≤–µ—Ä–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ %s, –æ–∂–∏–¥–∞–µ—Ç—Å—è %s", q_emb.shape, (mat.shape[1],))
        return []
    q_norm = np.linalg.norm(q_emb)
    if q_norm == 0:
        logger.error("semantic_search_in_memory: –Ω—É–ª–µ–≤–∞—è –Ω–æ—Ä–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞")
        return []
    q_emb = q_emb / q_norm
    scores = mat @ q_emb  # shape (n,)
    idx = np.argsort(-scores)[:k]
    results = []
    for i in idx:
        d = docs[int(i)].copy()
        d["score"] = float(scores[int(i)])
        results.append(d)
    return results


AUDIT_FILE = os.path.join(VECTORSTORE_DIR, "moderation_audit.log")
def audit_log(entry: dict):
    entry_out = {"ts": time.time(), **entry}
    with open(AUDIT_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry_out, ensure_ascii=False) + "\n")

# --- RAG pipeline: answer_user_query (sync) + async wrapper ---
def generate_mood_based_cocktail(query: str, context: str = "", max_tokens: int = 400, temp: float = 0.3) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–∫—Ç–µ–π–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —ç–º–æ—Ü–∏—è–º –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—é.
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    mood_mapping = {
        "–≤–µ—Å–µ–ª–æ–µ": "—è—Ä–∫–∏–π, —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π, –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–π",
        "—Å–ø–æ–∫–æ–π–Ω–æ–µ": "–º—è–≥–∫–∏–π, —É—Å–ø–æ–∫–∞–∏–≤–∞—é—â–∏–π, —Ä–∞—Å—Å–ª–∞–±–ª—è—é—â–∏–π",
        "—ç–Ω–µ—Ä–≥–∏—á–Ω–æ–µ": "–±–æ–¥—Ä—è—â–∏–π, –æ—Å–≤–µ–∂–∞—é—â–∏–π, —Ç–æ–Ω–∏–∑–∏—Ä—É—é—â–∏–π",
        "—Ä–æ–º–∞–Ω—Ç–∏—á–Ω–æ–µ": "–∏–∑—ã—Å–∫–∞–Ω–Ω—ã–π, —ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–π, —á—É–≤—Å—Ç–≤–µ–Ω–Ω—ã–π",
        "—É–≤–µ—Ä–µ–Ω–Ω–æ–µ": "–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π, —Å—Ç–∏–ª—å–Ω—ã–π, –≤—ã–¥–µ—Ä–∂–∞–Ω–Ω—ã–π",
        "—Ä–∞—Å—Å–ª–∞–±–ª–µ–Ω–Ω–æ–µ": "–ª–µ–≥–∫–∏–π, –æ—Å–≤–µ–∂–∞—é—â–∏–π, –Ω–µ–Ω–∞–≤—è–∑—á–∏–≤—ã–π"
    }

    mood_description = "–æ—Å–≤–µ–∂–∞—é—â–∏–π –∏ –ø—Ä–∏—è—Ç–Ω—ã–π"
    for mood, description in mood_mapping.items():
        if mood in query.lower():
            mood_description = description
            break

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —ç–º–æ–¥–∑–∏
    emoji_mapping = {
        "üòä": "—è—Ä–∫–∏–π, —Ä–∞–¥–æ—Å—Ç–Ω—ã–π, –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–π",
        "üòå": "–º—è–≥–∫–∏–π, —É—Å–ø–æ–∫–∞–∏–≤–∞—é—â–∏–π, –≥–∞—Ä–º–æ–Ω–∏—á–Ω—ã–π",
        "üî•": "–æ—Å—Ç—Ä—ã–π, —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π, —Å–æ–≥—Ä–µ–≤–∞—é—â–∏–π",
        "üí≠": "–Ω–µ–∂–Ω—ã–π, —Ä–æ–º–∞–Ω—Ç–∏—á–Ω—ã–π, –∏–∑—ã—Å–∫–∞–Ω–Ω—ã–π",
        "üòé": "—Å—Ç–∏–ª—å–Ω—ã–π, –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π, —É–≤–µ—Ä–µ–Ω–Ω—ã–π",
        "üåä": "–æ—Å–≤–µ–∂–∞—é—â–∏–π, –ª–µ–≥–∫–∏–π, –º–æ—Ä—Å–∫–æ–π"
    }

    for emoji, description in emoji_mapping.items():
        if emoji in query:
            mood_description = description
            break

    context_part = f"\n–î–æ—Å—Ç—É–ø–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n{context}\n" if context.strip() else ""

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç + —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è mood-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    SYSTEM_PROMPT = (
        SYSTEM_PROMPT_BARTENDER +
        "\n\n–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (–¥–ª—è –Ω–∞–ø–∏—Ç–∫–∞ –ø–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—é):\n"
        "üç∏ –ù–ê–ó–í–ê–ù–ò–ï –ù–ê–ü–ò–¢–ö–ê\n\n"
        "üé≠ –ü–æ—á–µ–º—É —ç—Ç–æ—Ç –Ω–∞–ø–∏—Ç–æ–∫ –∏–¥–µ–∞–ª–µ–Ω –¥–ª—è –≤–∞—à–µ–≥–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è:\n"
        "[1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ —Ç–æ–º, –∫–∞–∫ –Ω–∞–ø–∏—Ç–æ–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—é]\n\n"
        "ü•É –ò–ù–ì–†–ï–î–ò–ï–ù–¢–´:\n"
        "- –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç 1 (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)\n"
        "- –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç 2 (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)\n"
        "- –∏ —Ç.–¥.\n\n"
        "üë®‚Äçüç≥ –ü–†–ò–ì–û–¢–û–í–õ–ï–ù–ò–ï:\n"
        "1. –®–∞–≥ 1\n"
        "2. –®–∞–≥ 2\n"
        "3. –®–∞–≥ 3\n\n"
        "üí° –°–û–í–ï–¢ –ë–ê–†–ú–ï–ù–ê:\n"
        "[–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ñ–∞–∫—Ç –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–æ–≤–µ—Ç]"
    )

    user_prompt = (
        f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç {mood_description} –Ω–∞–ø–∏—Ç–æ–∫. "
        f"–ï–≥–æ –∑–∞–ø—Ä–æ—Å: \"{query}\"\n"
        f"{context_part}"
        f"–ü–æ–¥–±–µ—Ä–∏ –∏–¥–µ–∞–ª—å–Ω—ã–π –Ω–∞–ø–∏—Ç–æ–∫ –ø–æ–¥ —ç—Ç–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ —Å–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ä–µ—Ü–µ–ø—Ç."
    )

    resp = yandex_completion(
        [{"role": "system", "text": SYSTEM_PROMPT}, {"role": "user", "text": user_prompt}],
        temperature=temp,
        max_tokens=max_tokens
    )

    if resp.get("error"):
        logger.error("generate_mood_based_cocktail: completion error %s", resp)
        return ""

    text = extract_text_from_yandex_completion(resp)
    if not text:
        logger.warning("generate_mood_based_cocktail: empty response")
        return ""

    # –û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    text = "\n".join([ln.rstrip() for ln in text.splitlines() if ln.strip()])
    if len(text) > 1200:
        text = text[:1200] + "..."

    return text

def answer_user_query_sync(user_text: str, user_id: int, k: int = 3) -> Tuple[str, dict]:
    meta = {"user_id": user_id, "query": user_text}
    # 1) pre-moderation
    try:
        ok_pre_res = pre_moderate_input(user_text)
        if not isinstance(ok_pre_res, tuple) or len(ok_pre_res) != 2:
            logger.warning("pre_moderate_input returned unexpected: %r", ok_pre_res)
            ok_pre, pre_meta = True, {"via": "fallback", "reason": "pre_moderation_bad_return"}
        else:
            ok_pre, pre_meta = ok_pre_res
    except Exception as e:
        logger.exception("pre_moderate_input raised: %s", e)
        ok_pre, pre_meta = True, {"via": "exception", "error": str(e)}

    meta["pre_moderation"] = pre_meta
    if not ok_pre:
        audit_log({"user_id": user_id, "action": "blocked_pre", "query": user_text, "meta": pre_meta})
        return ("–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –º–æ–≥—É –ø–æ–º–æ—á—å —Å —ç—Ç–∏–º –∑–∞–ø—Ä–æ—Å–æ–º.", {"blocked": True, "reason": pre_meta})

    # 2) retrieval
    try:
        docs = semantic_search_in_memory(user_text, k=k)
    except Exception as e:
        logger.exception("semantic_search_in_memory failed: %s", e)
        docs = []

    meta["retrieved_count"] = len(docs)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–∏/—ç–º–æ—Ü–∏—è—Ö
    mood_keywords = ["–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ", "–≤–µ—Å–µ–ª–æ–µ", "—Å–ø–æ–∫–æ–π–Ω–æ–µ", "—ç–Ω–µ—Ä–≥–∏—á–Ω–æ–µ", "—Ä–æ–º–∞–Ω—Ç–∏—á–Ω–æ–µ",
                     "—É–≤–µ—Ä–µ–Ω–Ω–æ–µ", "—Ä–∞—Å—Å–ª–∞–±–ª–µ–Ω–Ω–æ–µ", "–≥—Ä—É—Å—Ç–Ω", "—Ä–∞–¥–æ—Å—Ç", "–∑–ª–æ—Å—Ç",
                     "—É—Å—Ç–∞–ª", "—Å—Ç—Ä–µ—Å—Å", "—Ä–∞—Å—Å–ª–∞–±", "–æ—Ç–¥–æ—Ö–Ω", "—Ä–µ–ª–∞–∫—Å"]
    is_mood_query = any(keyword in user_text.lower() for keyword in mood_keywords) or \
                    any(emoji in user_text for emoji in ["üòä", "üòå", "üî•", "üí≠", "üòé", "üåä"])

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    relevant_docs = [d for d in docs if d.get("score", 0) > 0.3]  # –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    has_good_context = len(relevant_docs) > 0

    # build context
    context_parts = []
    for d in relevant_docs:
        src = d.get("meta", {}).get("source", d.get("id", "unknown"))
        txt = d.get("text", "")
        context_parts.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {src}\n{txt}")
    context_for_model = "\n\n---\n\n".join(context_parts) if context_parts else ""

    # 3) call Yandex completion
    if is_mood_query or not has_good_context:
        # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—é –∏–ª–∏ –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–∫—Ç–µ–π–ª—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: %s (mood_query=%s, good_context=%s)",
                   user_text[:50], is_mood_query, has_good_context)
        answer = generate_mood_based_cocktail(user_text, context_for_model)
        if not answer:
            answer = generate_compact_cocktail(user_text)
        if not answer:
            answer = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
    else:
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        system_prompt = SYSTEM_PROMPT_BARTENDER
        user_prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n{context_for_model}\n\n–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_text}\n–û—Ç–≤–µ—Ç—å –∫–∞–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –±–∞—Ä–º–µ–Ω: —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, —Ä–µ—Ü–µ–ø—Ç—ã, —Å–æ–≤–µ—Ç—ã."
        yresp = yandex_completion([{"role": "system", "text": system_prompt}, {"role": "user", "text": user_prompt}])
        answer = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ–π—á–∞—Å –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞."
        if not yresp.get("error"):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ Yandex API
            answer = extract_text_from_yandex_completion(yresp)
            if not answer:
                # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–∫—Ç–µ–π–ª–µ–π
                answer = generate_compact_cocktail(user_text)
            if not answer:
                answer = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."

    meta["raw_response_preview"] = answer[:500]
    meta["used_mood_generation"] = is_mood_query or not has_good_context

    # 4) post moderation
    ok_post, post_meta = post_moderate_output(answer)
    meta["post_moderation"] = post_meta
    if not ok_post:
        audit_log({"user_id": user_id, "action": "blocked_post", "query": user_text, "raw_answer": answer[:400], "meta": post_meta})
        return ("–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –º–æ–≥—É –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —ç—Ç–æ—Ç –æ—Ç–≤–µ—Ç –ø–æ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.", {"blocked": True, "reason": post_meta})
    # 5) success
    audit_log({"user_id": user_id, "action": "answered", "query": user_text, "retrieved": [d.get("id") for d in docs], "meta": meta})
    return (answer, {"blocked": False, **meta})

def generate_compact_cocktail(query: str, max_tokens: int = 220, temp: float = 0.2) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π —Ä–µ—Ü–µ–ø—Ç –≤ —Å—Ç—Ä–æ–≥–æ –∑–∞–¥–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.
    query: —Å—Ç—Ä–æ–∫–∞ —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–Ω–∞–ø—Ä. "—Å–ª–∞–¥–∫–æ–µ, –±–µ–∑–∞–ª–∫–æ–≥–æ–ª—å–Ω–æ–µ")
    """
    SYSTEM_PROMPT_PERSONA = (
        SYSTEM_PROMPT_BARTENDER +
        "\n\n–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –Ω–µ –±–æ–ª–µ–µ 700 —Å–∏–º–≤–æ–ª–æ–≤. –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –Ω–∏–∂–µ (–±–µ–∑ –ª–∏—à–Ω–∏—Ö –≤–≤–æ–¥–Ω—ã—Ö):\n\n"
        "–ö–æ–∫—Ç–µ–π–ª—å: \"–ù–ê–ó–í–ê–ù–ò–ï\"\n"
        "–ò–ù–ì–†–ï–î–ò–ï–ù–¢–´:\n"
        "  - ...\n"
        "  - ...\n"
        "–ü–†–ò–ì–û–¢–û–í–õ–ï–ù–ò–ï:\n"
        "  - —à–∞–≥ 1\n"
        "  - —à–∞–≥ 2\n"
        "–ò–ù–¢–ï–†–ï–°–ù–´–ô –§–ê–ö–¢: –û–¥–Ω–æ-–¥–≤–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n"
        "–ù–∏ —Å—Ç—Ä–æ—á–µ–∫ –ª–∏—à–Ω–∏—Ö ‚Äî —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç —à–∞–±–ª–æ–Ω. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –ø—Ä–µ–¥–ª–æ–∂–∏ –∑–∞–º–µ–Ω—É –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞ –≤ —Å–∫–æ–±–∫–∞—Ö."
    )
    user = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {query}. –û—Ç–≤–µ—Ç—å –∫–æ—Ä–æ—Ç–∫–æ, –º–∞–∫—Å–∏–º—É–º 4 –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞, –º–∞–∫—Å–∏–º—É–º 4 —à–∞–≥–∞."
    resp = yandex_completion([{"role": "system", "text": SYSTEM_PROMPT_PERSONA}, {"role": "user", "text": user}], temperature=temp, max_tokens=max_tokens)
    if resp.get("error"):
        logger.error("generate_compact_cocktail: completion error %s", resp)
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—Ü–µ–ø—Ç."
    text = extract_text_from_yandex_completion(resp)
    if not text:
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—Ü–µ–ø—Ç."
    return text

async def async_answer_user_query(user_text: str, user_id: int, k: int = 3) -> Tuple[str, dict]:
    """
    Async wrapper: –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ä–∞–±–æ—Ç—É –≤ ThreadPoolExecutor,
    –±–µ–∑–æ–ø–∞—Å–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ async handle_message.
    """
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, lambda: answer_user_query_sync(user_text, user_id, k))

# --- Small utility for testing: add docs and build index ---
def build_index_from_plain_texts(text_docs: List[Tuple[str, str]], embedding_model_uri: Optional[str] = None):
    """
    text_docs: list of (id, text). Stores meta minimal.
    """
    docs = []
    for id_, txt in text_docs:
        docs.append({"id": id_, "text": txt, "meta": {"source": id_}})
    build_vectorstore_from_docs(docs, embedding_model_uri=embedding_model_uri)
    logger.info("Index built from %d texts", len(text_docs))
