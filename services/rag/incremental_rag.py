# incremental_rag.py - –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ RAG –∏–Ω–¥–µ–∫—Å–∞
import os
import json
import logging
import hashlib
from typing import List, Dict, Tuple
import boto3
import numpy as np
from datetime import datetime

from settings import VECTORSTORE_DIR, S3_ENDPOINT, S3_ACCESS_KEY, S3_SECRET_KEY
from services.rag.embending import yandex_batch_embeddings

logger = logging.getLogger(__name__)

INCREMENTAL_STATE_FILE = os.path.join(VECTORSTORE_DIR, "incremental_state.json")
VECTORS_FILE = "vectors.npy"
METADATA_FILE = "metadata.json"

def get_file_hash(content: bytes) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç MD5 —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
    return hashlib.md5(content).hexdigest()

def load_incremental_state() -> Dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    if os.path.exists(INCREMENTAL_STATE_FILE):
        try:
            with open(INCREMENTAL_STATE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ: %s", e)
    
    return {"processed_files": {}, "last_update": None}

def save_incremental_state(state: Dict):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    try:
        os.makedirs(os.path.dirname(INCREMENTAL_STATE_FILE), exist_ok=True)
        with open(INCREMENTAL_STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
    except Exception as e:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ: %s", e)

def get_s3_client():
    """–°–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å S3"""
    return boto3.client(
        "s3",
        endpoint_url=S3_ENDPOINT,
        aws_access_key_id=S3_ACCESS_KEY,
        aws_secret_access_key=S3_SECRET_KEY,
    )

def download_file_bytes(bucket_name: str, key: str) -> bytes:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –∏–∑ S3"""
    s3 = get_s3_client()
    response = s3.get_object(Bucket=bucket_name, Key=key)
    return response['Body'].read()

def get_bucket_files(bucket_name: str) -> List[Dict]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –±–∞–∫–µ—Ç–∞"""
    try:
        s3 = get_s3_client()
        response = s3.list_objects_v2(Bucket=bucket_name)
        contents = response.get("Contents", [])
        
        return [
            {
                "key": obj["Key"],
                "last_modified": obj["LastModified"].isoformat(),
                "size": obj["Size"]
            }
            for obj in contents 
            if obj["Key"].lower().endswith(('.csv', '.pdf', '.txt', '.json'))
        ]
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ñ–∞–π–ª–æ–≤ –∏–∑ %s: %s", bucket_name, e)
        return []

def find_new_files(bucket_name: str, state: Dict) -> List[Dict]:
    """–ù–∞—Ö–æ–¥–∏—Ç –Ω–æ–≤—ã–µ –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"""
    current_files = get_bucket_files(bucket_name)
    processed_files = state.get("processed_files", {})
    
    return [
        file_info for file_info in current_files
        if (file_info["key"] not in processed_files or 
            processed_files[file_info["key"]].get("last_modified") != file_info["last_modified"])
    ]

def extract_text_simple(content: bytes, filename: str) -> str:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
    # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ - –ø—Ä—è–º–æ–µ —á—Ç–µ–Ω–∏–µ
    if filename.lower().endswith('.txt'):
        return content.decode('utf-8', errors='ignore')
    
    # –î–ª—è CSV - –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥
    elif filename.lower().endswith('.csv'):
        text = content.decode('utf-8', errors='ignore')
        lines = text.split('\n')
        return ' '.join([line.strip() for line in lines if line.strip()])
    
    # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
    else:
        logger.warning("–§–æ—Ä–º–∞—Ç %s —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏", filename)
        return f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ {filename}"

def chunk_text_simple(text: str, max_chars: int = 1000) -> List[str]:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_length = 0
    
    for word in words:
        if current_length + len(word) + 1 > max_chars and current_chunk:
            chunks.append(' '.join(current_chunk))
            current_chunk = [word]
            current_length = len(word)
        else:
            current_chunk.append(word)
            current_length += len(word) + 1
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def process_file_simple(content: bytes, filename: str) -> List[Dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –∏ —Å–æ–∑–¥–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    try:
        text = extract_text_simple(content, filename)
        if not text.strip():
            return []
        
        chunks = chunk_text_simple(text)
        documents = []
        
        for i, chunk in enumerate(chunks):
            documents.append({
                "id": f"{filename}__{i}",
                "text": chunk,
                "meta": {
                    "source": filename,
                    "chunk": i,
                    "total_chunks": len(chunks),
                    "processed_at": datetime.now().isoformat()
                }
            })
        
        logger.info("–°–æ–∑–¥–∞–Ω–æ %d —á–∞–Ω–∫–æ–≤ –∏–∑ %s", len(documents), filename)
        return documents
        
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ %s: %s", filename, e)
        return []

def load_existing_index() -> Tuple[any, List, List]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å"""
    vectors_path = os.path.join(VECTORSTORE_DIR, VECTORS_FILE)
    metadata_path = os.path.join(VECTORSTORE_DIR, METADATA_FILE)
    
    if not os.path.exists(vectors_path) or not os.path.exists(metadata_path):
        return None, [], []
    
    try:
        vectors = np.load(vectors_path)
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        return vectors, vectors, metadata
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: %s", e)
        return None, [], []

def save_index(vectors: np.ndarray, metadata: List[Dict]):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫"""
    try:
        os.makedirs(VECTORSTORE_DIR, exist_ok=True)
        np.save(os.path.join(VECTORSTORE_DIR, VECTORS_FILE), vectors)
        with open(os.path.join(VECTORSTORE_DIR, METADATA_FILE), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error("–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: %s", e)
        raise

def update_rag_incremental(bucket_name: str) -> bool:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    
    Returns:
        bool: True –µ—Å–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞
    """
    try:
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –≤ –±–∞–∫–µ—Ç–µ %s", bucket_name)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
        state = load_incremental_state()
        existing_index, existing_vectors, existing_metadata = load_existing_index()
        
        if existing_index is None:
            logger.info("üìö –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞")
            return False
        
        # –ò—â–µ–º –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        new_files = find_new_files(bucket_name, state)
        if not new_files:
            logger.info("‚úÖ –ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return True
        
        logger.info("üì¶ –ù–∞–π–¥–µ–Ω–æ %d –Ω–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤", len(new_files))
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        all_new_docs = []
        all_new_vectors = []
        
        for file_info in new_files:
            try:
                content = download_file_bytes(bucket_name, file_info["key"])
                docs = process_file_simple(content, file_info["key"])
                
                if docs:
                    texts = [doc["text"] for doc in docs]
                    vectors = yandex_batch_embeddings(texts)
                    
                    if vectors and len(vectors) == len(docs):
                        all_new_docs.extend(docs)
                        all_new_vectors.extend(vectors)
                        logger.info("‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω %s: %d –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", file_info["key"], len(docs))
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                    state["processed_files"][file_info["key"]] = {
                        "hash": get_file_hash(content),
                        "last_modified": file_info["last_modified"]
                    }
                    
            except Exception as e:
                logger.error("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ %s: %s", file_info["key"], e)
                continue
        
        if not all_new_docs:
            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã")
            return True
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ —Å—Ç–∞—Ä—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        logger.info("üîó –û–±—ä–µ–¥–∏–Ω—è–µ–º %d –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ %d", 
                   len(all_new_docs), len(existing_metadata))
        
        all_vectors = np.vstack([existing_vectors, np.array(all_new_vectors, dtype=np.float32)])
        all_metadata = existing_metadata + all_new_docs
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        save_index(all_vectors, all_metadata)
        state["last_update"] = datetime.now().isoformat()
        save_incremental_state(state)
        
        logger.info("‚úÖ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: %d", len(all_metadata))
        return True
        
    except Exception as e:
        logger.error("‚ùå –§–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: %s", e)
        return False